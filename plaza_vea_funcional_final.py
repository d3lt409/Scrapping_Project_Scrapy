#!/usr/bin/env python3
"""
PlazaVea Scraper FINAL - Extiende la configuraci√≥n que funciona con selecci√≥n avanzada
"""

import os
import sys
import asyncio

# Agregar el directorio ra√≠z al path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def get_categories_from_constants():
    """Obtener categor√≠as desde las constantes de PlazaVea"""
    try:
        sys.path.append(os.path.join(os.getcwd(), 'scraper_vea'))
        from scraper_vea.spiders.constants import plaza_vea
        
        categories = {}
        
        for categoria, subcategorias in plaza_vea.CATEGORIAS_MERCADO.items():
            display_name = categoria.replace('_', ' ').replace('-', ' ').title()
            categories[categoria] = {
                'display_name': display_name,
                'subcategorias': subcategorias,
                'urls': []
            }
            
            for subcategoria in subcategorias:
                url = f"https://www.plazavea.com.pe/{categoria}/{subcategoria}"
                categories[categoria]['urls'].append({
                    'url': url,
                    'subcategoria': subcategoria,
                    'display_name': subcategoria.replace('-', ' ').title()
                })
        
        return categories
    except ImportError as e:
        print(f"‚ùå Error importando constantes: {e}")
        return {}

def show_categories(categories):
    """Mostrar categor√≠as disponibles"""
    print("üìã CATEGOR√çAS DISPONIBLES:")
    print("=" * 60)
    
    for i, (key, data) in enumerate(categories.items(), 1):
        subcategory_count = len(data['subcategorias'])
        print(f"{i:2d}. {data['display_name']} ({subcategory_count} subcategor√≠as)")
    
    print(f"\nüìä Total: {len(categories)} categor√≠as principales")
    print("=" * 60)

def show_subcategories(category_data, category_name):
    """Mostrar subcategor√≠as de una categor√≠a espec√≠fica"""
    print(f"\nüìã SUBCATEGOR√çAS DE '{category_name.upper()}':")
    print("=" * 60)
    
    for i, url_data in enumerate(category_data['urls'], 1):
        print(f"{i:2d}. {url_data['display_name']}")
    
    print(f"\nüìä Total: {len(category_data['urls'])} subcategor√≠as")
    print("=" * 60)

def select_subcategories(category_data, category_name):
    """Permitir selecci√≥n de subcategor√≠as espec√≠ficas"""
    show_subcategories(category_data, category_name)
    
    print("\nüéØ OPCIONES DE SELECCI√ìN:")
    print("‚Ä¢ 'all' o 'todos' para todas las subcategor√≠as")
    print("‚Ä¢ N√∫mero espec√≠fico (ej: 1, 3, 5)")
    print("‚Ä¢ Rango (ej: 1-3)")
    print("‚Ä¢ M√∫ltiples (ej: 1,3,5)")
    
    try:
        selection = input(f"\nSelecciona subcategor√≠as (1-{len(category_data['urls'])} o 'all'): ").strip().lower()
        
        if selection in ['all', 'todos', 'todas']:
            selected_urls = [url_data['url'] for url_data in category_data['urls']]
            selected_names = [url_data['display_name'] for url_data in category_data['urls']]
            print(f"‚úÖ Seleccionadas: TODAS las subcategor√≠as ({len(selected_urls)})")
            return selected_urls, selected_names
        
        # Parsear selecci√≥n
        selected_indices = []
        
        if '-' in selection:
            # Rango
            try:
                start, end = map(int, selection.split('-'))
                selected_indices = list(range(start, end + 1))
            except:
                print("‚ùå Formato de rango inv√°lido")
                return [], []
        elif ',' in selection:
            # M√∫ltiples
            try:
                selected_indices = [int(x.strip()) for x in selection.split(',')]
            except:
                print("‚ùå Formato de lista inv√°lido")
                return [], []
        else:
            # N√∫mero √∫nico
            try:
                selected_indices = [int(selection)]
            except:
                print("‚ùå N√∫mero inv√°lido")
                return [], []
        
        # Validar √≠ndices y crear listas
        selected_urls = []
        selected_names = []
        
        for idx in selected_indices:
            if 1 <= idx <= len(category_data['urls']):
                url_data = category_data['urls'][idx - 1]
                selected_urls.append(url_data['url'])
                selected_names.append(url_data['display_name'])
            else:
                print(f"‚ö†Ô∏è √çndice {idx} fuera de rango")
        
        if selected_urls:
            print(f"‚úÖ Seleccionadas {len(selected_urls)} subcategor√≠as:")
            for name in selected_names:
                print(f"   ‚Ä¢ {name}")
        
        return selected_urls, selected_names
        
    except Exception as e:
        print(f"‚ùå Error en selecci√≥n: {e}")
        return [], []

def ejecutar_scraper_funcionando(urls, categoria_nombre):
    """Ejecutar usando la configuraci√≥n EXACTA que funciona de plaza_vea_test_rapido.py"""
    
    print("üçé PLAZA VEA SCRAPER - CONFIGURACI√ìN QUE FUNCIONA")
    print("=" * 50)
    print(f"üìÇ Categor√≠a: {categoria_nombre}")
    print(f"üîó URLs a procesar: {len(urls)}")
    for i, url in enumerate(urls, 1):
        subcategoria = url.split('/')[-1].replace('-', ' ').title()
        print(f"   {i}. {subcategoria}")
    print("üöÄ Iniciando crawler...")
    
    # Configuraci√≥n EXACTA de plaza_vea_test_rapido.py que S√ç FUNCIONA
    settings = {
        'BOT_NAME': 'scraper_vea',
        'SPIDER_MODULES': ['scraper_vea.spiders'],
        'NEWSPIDER_MODULE': 'scraper_vea.spiders',
        
        # Configuraci√≥n b√°sica
        'ROBOTSTXT_OBEY': False,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
        
        # Scrapy-Playwright
        'DOWNLOAD_HANDLERS': {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        'TWISTED_REACTOR': "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
        
        # Delays
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        
        # Pipeline habilitado para guardar en base de datos (PlazaVea - Per√∫)
        'ITEM_PIPELINES': {
            'shared_pipeline.UnifiedPostgresPipeline': 300,
        },
        
        # Configuraci√≥n espec√≠fica para PlazaVea (Per√∫)
        'PIPELINE_TABLE_NAME': 'peru',
        'PIPELINE_COMERCIAL_NAME': 'PlazaVea',
        'PIPELINE_COMERCIAL_ID': 'plazavea_peru',
        
        # Logging
        'LOG_LEVEL': 'INFO',
        
        # Playwright settings
        'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            "headless": True,
            "timeout": 30000,
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 30000,
        
        # Windows
        'TELNETCONSOLE_ENABLED': False,
        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
        
        # L√≠mite removido para procesamiento completo
        # 'CLOSESPIDER_ITEMCOUNT': 10,  # Comentado para obtener TODO
    }
    
    try:
        # Configurar asyncio para Windows (igual que en plaza_vea_test_rapido.py)
        if sys.platform == 'win32':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
        # Importar CrawlerProcess (igual que en plaza_vea_test_rapido.py)
        from scrapy.crawler import CrawlerProcess
        
        # Crear proceso crawler (igual que en plaza_vea_test_rapido.py)
        process = CrawlerProcess(settings)
        
        # Convertir URLs a string para el spider
        urls_str = ",".join(urls)
        
        # Ejecutar spider (igual que en plaza_vea_test_rapido.py)
        process.crawl(
            'plaza_vea',  # nombre del spider
            custom_urls=urls_str  # par√°metro - m√∫ltiples URLs separadas por coma
        )
        
        # Ejecutar (igual que en plaza_vea_test_rapido.py)
        process.start()
        
        print("‚úÖ Scraping completado")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Funci√≥n principal que extiende plaza_vea_test_rapido.py con selecci√≥n avanzada"""
    
    print("üçé PLAZA VEA SCRAPER FINAL FUNCIONAL")
    print("   üéØ Selecci√≥n avanzada + Configuraci√≥n que FUNCIONA")
    print("=" * 60)
    
    # Obtener categor√≠as desde constantes
    categories = get_categories_from_constants()
    
    if not categories:
        print("‚ùå No se pudieron cargar las categor√≠as")
        return
    
    # Mostrar categor√≠as
    show_categories(categories)
    
    try:
        # Selecci√≥n de categor√≠a
        selection = input(f"\nSelecciona categor√≠a (1-{len(categories)}): ").strip()
        
        try:
            category_index = int(selection) - 1
            if not (0 <= category_index < len(categories)):
                print("‚ùå N√∫mero de categor√≠a inv√°lido")
                return
        except ValueError:
            print("‚ùå Entrada inv√°lida")
            return
        
        # Obtener categor√≠a seleccionada
        category_key = list(categories.keys())[category_index]
        category_data = categories[category_key]
        category_name = category_data['display_name']
        
        print(f"\n‚úÖ Categor√≠a seleccionada: {category_name}")
        
        # Selecci√≥n de subcategor√≠as
        selected_urls, selected_names = select_subcategories(category_data, category_name)
        
        if not selected_urls:
            print("‚ùå No se seleccionaron subcategor√≠as v√°lidas")
            return
        
        print(f"\nüìä RESUMEN:")
        print(f"   üìÇ Categor√≠a: {category_name}")
        print(f"   üì¶ Subcategor√≠as: {len(selected_urls)}")
        print(f"   üîó URLs a procesar: {len(selected_urls)}")
        
        confirm = input(f"\n¬øProcesar {len(selected_urls)} subcategor√≠a(s)? (s/n): ").lower().strip()
        if confirm not in ['s', 'si', 'y', 'yes']:
            print("‚ùå Operaci√≥n cancelada")
            return
            
        # Ejecutar scraper usando configuraci√≥n que funciona
        ejecutar_scraper_funcionando(selected_urls, f"{category_name} - {', '.join(selected_names)}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Interrumpido por el usuario")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()