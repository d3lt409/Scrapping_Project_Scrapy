import re
import scrapy
from scrapy_playwright.page import PageMethod
from scraper.items import ScraperItem
from scrapy.http import Response
from playwright.async_api import Page
import time
import json
import os
import subprocess
from datetime import datetime, timedelta

from .constants import inkafarma

class InkafarmaSpider(scrapy.Spider):
    name = "inkafarma"
    allowed_domains = ["inkafarma.pe"]
    
    def __init__(self, custom_urls=None, *args, **kwargs):
        super(InkafarmaSpider, self).__init__(*args, **kwargs)
        self.custom_urls = custom_urls  # Initialize custom_urls
        
        # Cargar o actualizar la estructura JSON
        self.structure_data = self._load_or_update_structure()
        
        # Procesar URLs de entrada bas√°ndose en la estructura JSON
        self.start_urls = self._process_json_structure()
    
    def _get_json_path(self):
        """Obtiene la ruta del archivo JSON de estructura"""
        return os.path.join(os.path.dirname(__file__), "constants", "inkafarma_complete_structure.json")
    
    def _is_json_outdated(self):
        """Verifica si el JSON es mayor a 1 mes de antig√ºedad"""
        json_path = self._get_json_path()
        
        if not os.path.exists(json_path):
            self.logger.info("üìã Archivo JSON no existe, necesita generarse")
            return True
        
        # Obtener fecha de modificaci√≥n del archivo
        file_mod_time = datetime.fromtimestamp(os.path.getmtime(json_path))
        one_month_ago = datetime.now() - timedelta(days=30)
        
        if file_mod_time < one_month_ago:
            self.logger.info(f"‚è∞ Archivo JSON desactualizado. √öltima modificaci√≥n: {file_mod_time}")
            return True
        
        self.logger.info(f"‚úÖ Archivo JSON actualizado. √öltima modificaci√≥n: {file_mod_time}")
        return False
    
    def _run_inkatest_spider(self):
        """Ejecuta el spider inkatest para generar/actualizar el JSON"""
        try:
            self.logger.info("üöÄ Ejecutando spider inkatest para actualizar estructura...")
            
            # Cambiar al directorio del proyecto para ejecutar scrapy
            current_dir = os.getcwd()
            project_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            
            os.chdir(project_dir)
            
            # Ejecutar el spider inkatest
            result = subprocess.run(
                ["scrapy", "crawl", "inkatest"],
                capture_output=True,
                text=True,
                timeout=7200  # 2 horas m√°ximo
            )
            
            os.chdir(current_dir)
            
            if result.returncode == 0:
                self.logger.info("‚úÖ Spider inkatest completado exitosamente")
                return True
            else:
                self.logger.error(f"‚ùå Error ejecutando inkatest: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error("‚è∞ Timeout ejecutando inkatest")
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Error ejecutando inkatest: {e}")
            return False
    
    def _load_structure_json(self):
        """Carga la estructura desde el archivo JSON"""
        json_path = self._get_json_path()
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.logger.info(f"üìä Estructura cargada: {data['metadata']['total_categories']} categor√≠as, "
                           f"{data['metadata']['total_subcategories']} subcategor√≠as, "
                           f"{data['metadata']['total_subsubcategories']} sub-subcategor√≠as")
            
            return data
            
        except Exception as e:
            self.logger.error(f"‚ùå Error cargando JSON: {e}")
            return None
    
    def _load_or_update_structure(self):
        """Carga la estructura JSON o la actualiza si es necesario"""
        
        # Verificar si necesita actualizaci√≥n
        if self._is_json_outdated():
            self.logger.info("üîÑ Actualizando estructura de categor√≠as...")
            
            if self._run_inkatest_spider():
                self.logger.info("‚úÖ Estructura actualizada exitosamente")
            else:
                self.logger.warning("‚ö†Ô∏è No se pudo actualizar, usando estructura existente")
        
        # Cargar la estructura (actualizada o existente)
        return self._load_structure_json()
    
    def _process_json_structure(self):
        """Procesa la estructura JSON para generar URLs de inicio"""
        if not self.structure_data:
            self.logger.error("‚ùå No hay estructura JSON disponible, usando fallback")
            return ["https://inkafarma.pe/"]
        
        # Si hay custom_urls, usarlas
        if self.custom_urls:
            urls = self.custom_urls.split(',') if isinstance(self.custom_urls, str) else self.custom_urls
            return [url.strip() for url in urls]
        
        # Generar URLs desde el JSON
        start_urls = []
        
        for category_key, category_data in self.structure_data["categories"].items():
            # Agregar URL de la categor√≠a principal
            category_url = f"https://inkafarma.pe{category_data['href']}"
            start_urls.append(category_url)
        
        self.logger.info(f"üéØ Generadas {len(start_urls)} URLs desde estructura JSON")
        return start_urls
    
    def _extract_category_info_from_url(self, url):
        """Extrae informaci√≥n de categor√≠a desde la URL usando la estructura JSON"""
        if not self.structure_data:
            return None
        
        # Extraer path de la URL
        path = url.replace("https://inkafarma.pe", "")
        
        # Buscar en la estructura JSON
        for category_key, category_data in self.structure_data["categories"].items():
            if category_data["href"] == path:
                return {
                    "level": "category",
                    "category_key": category_key,
                    "category_name": category_data["name"],
                    "href": category_data["href"],
                    "subcategories": category_data["subcategories"]
                }
        
        return None
    
    def _get_subcategory_urls(self, category_info):
        """Obtiene URLs de subcategor√≠as para una categor√≠a dada"""
        urls = []
        
        if category_info and "subcategories" in category_info:
            for sub_key, sub_data in category_info["subcategories"].items():
                url = f"https://inkafarma.pe{sub_data['href']}"
                urls.append({
                    "url": url,
                    "category_name": category_info["category_name"],
                    "subcategory_name": sub_data["name"],
                    "subcategory_key": sub_key,
                    "href": sub_data["href"],
                    "subsubcategories": sub_data.get("subsubcategories", {})
                })
        
        return urls
    
    def _get_subsubcategory_urls(self, subcategory_info):
        """Obtiene URLs de sub-subcategor√≠as para una subcategor√≠a dada"""
        urls = []
        
        if "subsubcategories" in subcategory_info:
            for subsub_key, subsub_data in subcategory_info["subsubcategories"].items():
                url = f"https://inkafarma.pe{subsub_data['href']}"
                urls.append({
                    "url": url,
                    "category_name": subcategory_info["category_name"],
                    "subcategory_name": subcategory_info["subcategory_name"],
                    "subsubcategory_name": subsub_data["name"],
                    "href": subsub_data["href"]
                })
        
        return urls
    
    async def _count_products_on_page(self, page):
        """Cuenta los productos en la p√°gina actual usando el H3 que muestra el conteo real"""
        try:
            # Esperar a que se cargue el contenido y el H3 con el conteo
            await page.wait_for_selector(inkafarma.SELECTOR_PRODUCT_COUNT_H3, timeout=10000)
            await page.wait_for_timeout(2000)
            
            # Obtener el texto del H3 que contiene el conteo de productos
            h3_element = page.locator(inkafarma.SELECTOR_PRODUCT_COUNT_H3)
            h3_text = await h3_element.text_content()
            
            if h3_text:
                # Extraer n√∫mero usando regex (ej: "Encontramos 156 productos")
                import re
                match = re.search(r'(\d+)', h3_text)
                if match:
                    count = int(match.group(1))
                    self.logger.info(f"üìä Conteo desde H3: '{h3_text}' -> {count} productos")
                    return count
            
            # Fallback: contar elementos DOM si no se encuentra el H3
            products = page.locator("//fp-filtered-product-list//fp-product-large")
            count = await products.count()
            self.logger.warning(f"‚ö†Ô∏è Usando conteo DOM fallback: {count}")
            return count
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Error contando productos: {e}")
            return 0
    
    async def parse_category_json(self, response):
        """Parser principal usando la estructura JSON"""
        page = response.meta["playwright_page"]
        category_info = response.meta["category_info"]
        
        if not category_info:
            self.logger.error(f"‚ùå No se pudo extraer informaci√≥n de categor√≠a para {response.url}")
            await page.close()
            return
        
        self.logger.info(f"üîç Procesando categor√≠a: {category_info['category_name']}")
        
        # Contar productos en la p√°gina actual
        product_count = await self._count_products_on_page(page)
        self.logger.info(f"üìä Productos encontrados en categor√≠a principal: {product_count}")
        
        # Si hay m√°s de 250 productos, navegar por subcategor√≠as
        if product_count >= 250:
            self.logger.info(f"‚ö†Ô∏è Categor√≠a con {product_count} productos (‚â•250). Navegando por subcategor√≠as...")
            
            # Generar requests para subcategor√≠as
            subcategory_urls = self._get_subcategory_urls(category_info)
            
            for sub_info in subcategory_urls:
                yield scrapy.Request(
                    url=sub_info["url"],
                    callback=self.parse_subcategory_json,
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "subcategory_info": sub_info,
                        "playwright_page_goto_kwargs": {
                            "wait_until": "domcontentloaded",
                            "timeout": 30000,
                        },
                        "playwright_page_methods": [
                            PageMethod("wait_for_load_state", "domcontentloaded"),
                            PageMethod("wait_for_timeout", 3000),
                        ],
                    }
                )
        else:
            # Procesar productos directamente
            async for item in self.parse_products(response, category_name="farmacia", subcategory_name=category_info['category_name']):
                yield item
        
        await page.close()
    
    async def parse_subcategory_json(self, response):
        """Parser para subcategor√≠as usando la estructura JSON"""
        page = response.meta["playwright_page"]
        subcategory_info = response.meta["subcategory_info"]
        
        self.logger.info(f"üîç Procesando subcategor√≠a: {subcategory_info['subcategory_name']}")
        
        # Contar productos en la subcategor√≠a
        product_count = await self._count_products_on_page(page)
        self.logger.info(f"üìä Productos encontrados en subcategor√≠a: {product_count}")
        
        # Si hay m√°s de 250 productos, navegar por sub-subcategor√≠as
        if product_count >= 250 and subcategory_info.get("subsubcategories"):
            self.logger.info(f"‚ö†Ô∏è Subcategor√≠a con {product_count} productos (‚â•250). Navegando por sub-subcategor√≠as...")
            
            # Generar requests para sub-subcategor√≠as
            subsubcategory_urls = self._get_subsubcategory_urls(subcategory_info)
            
            for subsub_info in subsubcategory_urls:
                yield scrapy.Request(
                    url=subsub_info["url"],
                    callback=self.parse_subsubcategory_json,
                    meta={
                        "playwright": True,
                        "playwright_include_page": True,
                        "subsubcategory_info": subsub_info,
                        "playwright_page_goto_kwargs": {
                            "wait_until": "domcontentloaded",
                            "timeout": 30000,
                        },
                        "playwright_page_methods": [
                            PageMethod("wait_for_load_state", "domcontentloaded"),
                            PageMethod("wait_for_timeout", 3000),
                        ],
                    }
                )
        else:
            # Procesar productos directamente
            async for item in self.parse_products(response, category_name="farmacia", subcategory_name=subcategory_info['subcategory_name']):
                yield item
        
        await page.close()
    
    async def parse_subsubcategory_json(self, response):
        """Parser para sub-subcategor√≠as usando la estructura JSON"""
        page = response.meta["playwright_page"]
        subsubcategory_info = response.meta["subsubcategory_info"]
        
        self.logger.info(f"üîç Procesando sub-subcategor√≠a: {subsubcategory_info['subsubcategory_name']}")
        
        # Procesar productos directamente (nivel m√°s profundo)
        async for item in self.parse_products(response, category_name="farmacia", subcategory_name=subsubcategory_info['subcategory_name']):
            yield item
        
        await page.close()
    
    def _process_input_urls(self, custom_urls):
        if custom_urls:
            urls = custom_urls.split(',') if isinstance(custom_urls, str) else custom_urls
            cleaned_urls = [url.strip() for url in urls]
            self.logger.info(f"Usando URLs personalizadas: {len(cleaned_urls)} URLs")
            return cleaned_urls, [None] * len(cleaned_urls)  # Return None for subcategories
        else:
            urls = []
            subcategories = []
            for categoria in inkafarma.CATEGORIAS:
                url = inkafarma.CATEGORIA_URL_TEMPLATE.format(categoria=categoria)
                urls.append(url)
                subcategories.append(categoria)
            self.logger.info(f"URLs generadas autom√°ticamente: {len(urls)} URLs")
            return urls, subcategories

    def generate_all_subcategory_urls(self):
        """
        Genera todas las URLs de subcategor√≠as usando la estructura JSON (preferred)
        √ötil para scraping exhaustivo por subcategor√≠as
        """
        all_urls = []
        # Preferir la estructura JSON si est√° disponible
        if self.structure_data and isinstance(self.structure_data.get('categories'), dict):
            for categoria_key, categoria_obj in self.structure_data['categories'].items():
                subcategories = categoria_obj.get('subcategories', {}) or {}
                for subcat_key, subcat_obj in subcategories.items():
                    href = subcat_obj.get('href') or f"/categoria/{categoria_key}/{subcat_key}"
                    url = f"https://inkafarma.pe{href}"
                    all_urls.append({
                        'url': url,
                        'categoria': categoria_key,
                        'subcategoria': subcat_key
                    })

            self.logger.info(f"üìã Generadas {len(all_urls)} URLs de subcategor√≠as desde JSON")
            return all_urls

        # Fallback a constantes si por alg√∫n motivo no existe el JSON
        for categoria, subcategorias in getattr(inkafarma, 'CATEGORIAS_CON_SUBCATEGORIAS', {}).items():
            for subcategoria in subcategorias:
                url = inkafarma.CATEGORIA_SUBCATEGORIA_URL_TEMPLATE.format(
                    categoria=categoria,
                    subcategoria=subcategoria
                )
                all_urls.append({
                    'url': url,
                    'categoria': categoria,
                    'subcategoria': subcategoria
                })

        self.logger.info(f"üìã Generadas {len(all_urls)} URLs de subcategor√≠as (fallback constantes)")
        return all_urls

    def start_requests(self):
        """Generar requests iniciales usando la estructura JSON"""
        self.logger.info("üöÄ Iniciando scraping de InkaFarma con estructura JSON...")

        for i, url in enumerate(self.start_urls):
            unique_url = f"{url}?scrapy_index={i}&ts={int(time.time())}"
            
            # Extraer informaci√≥n de categor√≠a desde la URL
            category_info = self._extract_category_info_from_url(url)
            
            yield scrapy.Request(
                url=unique_url,
                callback=self.parse_category_json,
                meta={
                    "playwright": True,
                    "playwright_include_page": True,
                    "category_info": category_info,
                    "playwright_page_goto_kwargs": {
                        "wait_until": "domcontentloaded",
                        "timeout": 30000,
                    },
                    "playwright_page_methods": [
                        PageMethod("wait_for_load_state", "domcontentloaded"),
                        PageMethod("wait_for_timeout", 5000),
                    ],
                },
                dont_filter=True
            )

    async def extract_menu_structure(self, page, category_prefix):
        """
        Extrae la estructura del men√∫ de InkaFarma usando una estrategia directa.
        En lugar de depender del hover, usa las categor√≠as conocidas de InkaFarma.
        """
        menu_structure = {}

        try:
            # Usar categor√≠as directas ya que el hover no funciona como esper√°bamos
            target_dept_url = f"https://inkafarma.pe/categoria/{category_prefix}"
            
            self.logger.info(f"üöÄ Navegando directamente al departamento: {target_dept_url}")
            await page.goto(target_dept_url, wait_until="domcontentloaded", timeout=30000)
            await page.wait_for_timeout(3000)
            
            # Verificar si la p√°gina carg√≥ correctamente
            page_title = await page.title()
            if "404" in page_title or "Not Found" in page_title:
                self.logger.warning(f"‚ö†Ô∏è Categor√≠a no encontrada: {target_dept_url}")
                return {}
            
            # Verificar si esta p√°gina tiene subcategor√≠as o productos directamente
            product_count = await self.get_product_count(page)
            self.logger.info(f"üìä Productos en departamento '{category_prefix}': {product_count}")
            
            if product_count >= 250:
                # Buscar subcategor√≠as en filtros laterales o en la p√°gina
                self.logger.info("üîç Buscando subcategor√≠as...")
                
                # Estrategia 1: Buscar enlaces de subcategor√≠as en filtros
                subcategory_links = await page.query_selector_all("a[href*='/categoria/']")
                categorias_data = []
                seen_urls = set()
                
                for subcat_link in subcategory_links:
                    href = await subcat_link.get_attribute("href")
                    text = await subcat_link.text_content()
                    
                    if href and href != f"/categoria/{category_prefix}" and category_prefix in href:
                        # Evitar duplicados
                        if href not in seen_urls:
                            seen_urls.add(href)
                            full_url = f"https://inkafarma.pe{href}" if href.startswith('/') else href
                            categorias_data.append({
                                "nombre": text.strip() if text else f"Subcategor√≠a {len(categorias_data)+1}",
                                "href": full_url,
                                "subcategorias": []
                            })
                self.logger.info(f"üìã Encontradas {len(categorias_data)} subcategor√≠as para '{category_prefix}'")
                
                if len(categorias_data) == 0:
                    self.logger.warning(f"‚ö†Ô∏è No se encontraron subcategor√≠as v√°lidas para '{category_prefix}'. Activando fallback...")
                    return {}  # Estructura vac√≠a para activar fallback
                
                menu_structure = {
                    "departamento": {
                        "nombre": category_prefix.replace("-", " ").title(),
                        "href": target_dept_url
                    },
                    "categorias": categorias_data
                }
            else:
                # Si tiene pocos productos, extraer directamente
                self.logger.info(f"‚úÖ Departamento '{category_prefix}' tiene {product_count} productos. Extrayendo directamente...")
                menu_structure = {
                    "departamento": {
                        "nombre": category_prefix.replace("-", " ").title(),
                        "href": target_dept_url
                    },
                    "categorias": [],
                    "extract_direct": True
                }

        except Exception as e:
            self.logger.error(f"‚ùå Error extrayendo estructura del men√∫ para '{category_prefix}': {e}")
            import traceback
            self.logger.error(f"üîç Traceback: {traceback.format_exc()}")

        return menu_structure

    async def parse_category(self, response):
        """Parsea una categor√≠a espec√≠fica - si >= 250 productos, extrae del men√∫ y navega directamente"""
        page = response.meta["playwright_page"]
        
        try:
            current_url = page.url
            self.logger.info(f"üöÄ Procesando categor√≠a: {current_url}")
            
            # Esperar a que se cargue la p√°gina inicial
            await page.wait_for_timeout(3000)

            # Verificar el n√∫mero de productos en la p√°gina
            product_count = await self.get_product_count(page)
            self.logger.info(f"üìä Productos encontrados en la categor√≠a: {product_count}")

            if product_count < 250:
                # Si hay menos de 250 productos, hacer scroll e extraer
                self.logger.info(f"‚úÖ {product_count} productos (<250). Extrayendo directamente...")
                await self.await_products_loaded(page)
                productos_cargados = await self.scroll_to_load_all_products(page)
                self.logger.info(f"‚úÖ Total productos cargados: {productos_cargados}")
                content = await page.content()
                await page.close()
                from scrapy.http import HtmlResponse
                updated_response = HtmlResponse(
                    url=response.url,
                    body=content,
                    encoding='utf-8'
                )
                # Extraer categor√≠a de la URL para usar como subcategor√≠a
                category_name = self.extract_category_from_url(response.url)
                for item in self.parse_products(updated_response, category_name):
                    yield item
            else:
                # Si hay >= 250 productos, extraer URLs del men√∫ y navegar a subcategor√≠as
                self.logger.info(f"üîÑ Detectados {product_count} productos (>=250). Buscando subcategor√≠as en el men√∫...")
                
                category_name = self.extract_category_from_url(current_url)
                category_prefix = category_name[:4].lower()
                self.logger.info(f"üîç Buscando departamento que empiece con: '{category_prefix}'")
                
                # Extraer toda la estructura del men√∫
                menu_structure = await self.extract_menu_structure(page, category_prefix)
                
                if not menu_structure or not menu_structure.get('categorias'):
                    self.logger.warning("‚ö†Ô∏è No se encontr√≥ estructura del men√∫ o departamentos. Activando fallback con subcategor√≠as predefinidas...")
                    
                    # NUEVO FALLBACK: Usar estructura JSON (preferible) o constantes como √∫ltima opci√≥n
                    category_raw = self.extract_category_raw_from_url(current_url)
                    self.logger.info(f"üîç Buscando categoria '{category_raw}' en estructura JSON...")

                    # Intentar obtener subcategor√≠as desde la estructura JSON
                    if self.structure_data and isinstance(self.structure_data.get('categories'), dict) and category_raw in self.structure_data['categories']:
                        subcats_data = self.structure_data['categories'][category_raw].get('subcategories', {}) or {}
                        self.logger.info(f"‚úÖ Encontrada categoria '{category_raw}' con {len(subcats_data)} subcategor√≠as (desde JSON)")

                        # Procesar cada subcategor√≠a usando los href del JSON
                        for subcat_key, subcat_obj in subcats_data.items():
                            try:
                                href = subcat_obj.get('href') or f"/categoria/{category_raw}/{subcat_key}"
                                subcategoria_url = f"https://inkafarma.pe{href}"
                                self.logger.info(f"üîÑ JSON Fallback: Procesando subcategor√≠a '{subcat_key}' ‚Üí {subcategoria_url}")

                                # Navegar a la subcategor√≠a
                                await page.goto(subcategoria_url, wait_until="domcontentloaded", timeout=30000)
                                await page.wait_for_timeout(3000)

                                # Verificar si la subcategor√≠a existe y tiene productos
                                page_title = await page.title()
                                if "404" not in page_title and "Not Found" not in page_title:
                                    subcat_count = await self.get_product_count(page)
                                    self.logger.info(f"  ÔøΩ Productos en subcategor√≠a '{subcat_key}': {subcat_count}")

                                    if subcat_count > 0:
                                        # Extraer productos de la subcategor√≠a
                                        await self.await_products_loaded(page)
                                        productos_cargados = await self.scroll_to_load_all_products(page)
                                        self.logger.info(f"  ‚úÖ Productos extra√≠dos de '{subcat_key}': {productos_cargados}")

                                        content = await page.content()
                                        from scrapy.http import HtmlResponse
                                        updated_response = HtmlResponse(
                                            url=subcategoria_url,
                                            body=content,
                                            encoding='utf-8'
                                        )
                                        for item in self.parse_products(updated_response, subcat_key):
                                            yield item
                                    else:
                                        self.logger.info(f"  ‚ö†Ô∏è Subcategor√≠a '{subcat_key}' sin productos")
                                else:
                                    self.logger.warning(f"  ‚ö†Ô∏è Subcategor√≠a no encontrada: {subcategoria_url}")

                            except Exception as subcat_error:
                                self.logger.error(f"  ‚ùå Error procesando subcategor√≠a '{subcat_key}': {subcat_error}")
                                continue

                        await page.close()
                        return

                    # Si no est√° en JSON, intentar fallback a las constantes antiguas
                    self.logger.info(f"üîç Intentando fallback con constantes para '{category_raw}'")
                    if hasattr(inkafarma, 'CATEGORIAS_CON_SUBCATEGORIAS') and category_raw in inkafarma.CATEGORIAS_CON_SUBCATEGORIAS:
                        subcategorias = inkafarma.CATEGORIAS_CON_SUBCATEGORIAS[category_raw]
                        self.logger.info(f"‚úÖ Encontrada categoria '{category_raw}' con {len(subcategorias)} subcategor√≠as (desde constantes)")

                        for subcategoria in subcategorias:
                            try:
                                subcategoria_url = inkafarma.CATEGORIA_SUBCATEGORIA_URL_TEMPLATE.format(
                                    categoria=category_raw,
                                    subcategoria=subcategoria
                                )
                                self.logger.info(f"üîÑ Constantes Fallback: Procesando subcategor√≠a '{subcategoria}' ‚Üí {subcategoria_url}")

                                await page.goto(subcategoria_url, wait_until="domcontentloaded", timeout=30000)
                                await page.wait_for_timeout(3000)

                                page_title = await page.title()
                                if "404" not in page_title and "Not Found" not in page_title:
                                    subcat_count = await self.get_product_count(page)
                                    self.logger.info(f"  üìä Productos en subcategor√≠a '{subcategoria}': {subcat_count}")

                                    if subcat_count > 0:
                                        await self.await_products_loaded(page)
                                        productos_cargados = await self.scroll_to_load_all_products(page)
                                        self.logger.info(f"  ‚úÖ Productos extra√≠dos de '{subcategoria}': {productos_cargados}")

                                        content = await page.content()
                                        from scrapy.http import HtmlResponse
                                        updated_response = HtmlResponse(
                                            url=subcategoria_url,
                                            body=content,
                                            encoding='utf-8'
                                        )
                                        for item in self.parse_products(updated_response, subcategoria):
                                            yield item
                                    else:
                                        self.logger.info(f"  ‚ö†Ô∏è Subcategor√≠a '{subcategoria}' sin productos")
                                else:
                                    self.logger.warning(f"  ‚ö†Ô∏è Subcategor√≠a no encontrada: {subcategoria_url}")

                            except Exception as subcat_error:
                                self.logger.error(f"  ‚ùå Error procesando subcategor√≠a '{subcategoria}': {subcat_error}")
                                continue

                        await page.close()
                        return
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Categor√≠a '{category_raw}' no encontrada en estructura (JSON ni constantes)")
                        if not self.structure_data:
                            self.logger.error("‚ùå Estructura JSON no est√° disponible para el fallback")
                    
                    # FALLBACK FINAL: Si no se encuentra en la estructura, scraping directo
                    original_url = response.url.split('?')[0]  # Remover par√°metros de scrapy
                    self.logger.info(f"üîÑ Fallback final: Navegando de vuelta a la URL original: {original_url}")
                    
                    try:
                        await page.goto(original_url, wait_until="domcontentloaded", timeout=30000)
                        await page.wait_for_timeout(3000)
                        
                        # Hacer scraping directo con scroll infinito
                        self.logger.info("üìú Fallback final: Iniciando scraping directo con scroll infinito...")
                        await self.await_products_loaded(page)
                        productos_cargados = await self.scroll_to_load_all_products(page)
                        self.logger.info(f"‚úÖ Fallback final completado: {productos_cargados} productos extra√≠dos")
                        
                        content = await page.content()
                        from scrapy.http import HtmlResponse
                        updated_response = HtmlResponse(
                            url=original_url,
                            body=content,
                            encoding='utf-8'
                        )
                        # Extraer categor√≠a de la URL para usar como subcategor√≠a
                        category_name = self.extract_category_from_url(original_url)
                        for item in self.parse_products(updated_response, category_name):
                            yield item
                            
                    except Exception as fallback_error:
                        self.logger.error(f"‚ùå Error en fallback final: {fallback_error}")
                    
                    await page.close()
                    return
                
                # Procesar cada categor√≠a del array temporal
                categorias = menu_structure.get('categorias', [])
                self.logger.info(f"üìã Procesando {len(categorias)} categor√≠as del men√∫...")
                
                for cat_info in categorias:
                    cat_nombre = cat_info.get('nombre', 'Sin nombre')
                    cat_href = cat_info.get('href', '')
                    subcategorias = cat_info.get('subcategorias', [])
                    
                    self.logger.info(f"üîÑ Procesando categor√≠a: '{cat_nombre}'")
                    
                    if subcategorias:
                        # Si hay subcategor√≠as, procesar cada una
                        self.logger.info(f"  üìä Encontradas {len(subcategorias)} subcategor√≠as")
                        
                        for subcat_info in subcategorias:
                            subcat_nombre = subcat_info.get('nombre', 'Sin nombre')
                            subcat_href = subcat_info.get('href', '')
                            
                            if not subcat_href:
                                self.logger.warning(f"  ‚ö†Ô∏è Subcategor√≠a sin href: {subcat_nombre}")
                                continue
                            
                            # Navegar a la subcategor√≠a
                            full_url = f"https://inkafarma.pe{subcat_href}" if subcat_href.startswith('/') else subcat_href
                            self.logger.info(f"  ‚û°Ô∏è Navegando a subcategor√≠a: '{subcat_nombre}' ‚Üí {full_url}")
                            
                            try:
                                await page.goto(full_url, wait_until="domcontentloaded", timeout=30000)
                                await page.wait_for_timeout(2000)
                                
                                # Verificar productos en esta subcategor√≠a
                                subcat_count = await self.get_product_count(page)
                                self.logger.info(f"  üìä Productos en '{subcat_nombre}': {subcat_count}")
                                
                                if subcat_count > 0:
                                    # Extraer productos
                                    await self.await_products_loaded(page)
                                    productos_cargados = await self.scroll_to_load_all_products(page)
                                    self.logger.info(f"  ‚úÖ Productos extra√≠dos: {productos_cargados}")
                                    
                                    content = await page.content()
                                    from scrapy.http import HtmlResponse
                                    updated_response = HtmlResponse(
                                        url=page.url,
                                        body=content,
                                        encoding='utf-8'
                                    )
                                    for item in self.parse_products(updated_response, subcat_nombre):
                                        yield item
                                
                            except Exception as e:
                                self.logger.error(f"  ‚ùå Error procesando subcategor√≠a '{subcat_nombre}': {e}")
                                continue
                    else:
                        # Si no hay subcategor√≠as, procesar la categor√≠a directamente
                        if not cat_href:
                            self.logger.warning(f"  ‚ö†Ô∏è Categor√≠a sin href: {cat_nombre}")
                            continue
                        
                        full_url = f"https://inkafarma.pe{cat_href}" if cat_href.startswith('/') else cat_href
                        self.logger.info(f"  ‚û°Ô∏è Navegando a categor√≠a: '{cat_nombre}' ‚Üí {full_url}")
                        
                        try:
                            await page.goto(full_url, wait_until="domcontentloaded", timeout=30000)
                            await page.wait_for_timeout(2000)
                            
                            cat_count = await self.get_product_count(page)
                            self.logger.info(f"  üìä Productos en '{cat_nombre}': {cat_count}")
                            
                            if cat_count > 0:
                                await self.await_products_loaded(page)
                                productos_cargados = await self.scroll_to_load_all_products(page)
                                self.logger.info(f"  ‚úÖ Productos extra√≠dos: {productos_cargados}")
                                
                                content = await page.content()
                                from scrapy.http import HtmlResponse
                                updated_response = HtmlResponse(
                                    url=page.url,
                                    body=content,
                                    encoding='utf-8'
                                )
                                for item in self.parse_products(updated_response, cat_nombre):
                                    yield item
                        
                        except Exception as e:
                            self.logger.error(f"  ‚ùå Error procesando categor√≠a '{cat_nombre}': {e}")
                            continue
                
                await page.close()

        except Exception as e:
            self.logger.error(f"‚ùå Error al procesar categor√≠a: {e}")
            if 'page' in locals():
                await page.close()

    async def get_product_count(self, page):
        """Obtener el n√∫mero de productos de la p√°gina usando el selector h3"""
        import re
        try:
            await page.wait_for_selector(inkafarma.SELECTOR_PRODUCT_COUNT_H3, timeout=3000)
            count_text = await page.text_content(inkafarma.SELECTOR_PRODUCT_COUNT_H3)
            match = re.search(r'(\d+)', count_text)
            if match:
                count = int(match.group(1))
                self.logger.info(f"üìù Encontrado en h3: {count} productos")
                return count
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è No se encontr√≥ h3 de conteo: {e}")
        self.logger.warning("‚ö†Ô∏è No se pudo obtener el conteo de productos")
        return 0

    async def navigate_subcategories(self, page, current_url):
        """Navegar por subcategor√≠as cuando hay >= 250 productos usando los selectores actualizados"""
        try:
            category_name = self.extract_category_from_url(current_url)
            category_prefix = category_name[:4].lower()
            self.logger.info(f"üîç Buscando subcategor√≠as que contengan: {category_prefix}")

            # Hacer clic en el men√∫ de categor√≠as
            await page.click(inkafarma.SELECTOR_CATEGORIES_MENU_BUTTON)
            await page.wait_for_timeout(2000)

            # Buscar subcategor√≠as
            subcategory_elements = await page.query_selector_all(inkafarma.SELECTOR_SUBCATEGORIES)
            self.logger.info(f"üìÇ Encontradas {len(subcategory_elements)} subcategor√≠as")

            for i, subcat in enumerate(subcategory_elements):
                # Obtener el texto del span dentro de la subcategor√≠a
                span = await subcat.query_selector(inkafarma.SELECTOR_SUBCATEGORY_SPAN)
                subcat_text = await span.text_content() if span else ""
                self.logger.info(f"üîÑ Subcategor√≠a {i+1}: {subcat_text}")
                # Verificar si el texto contiene el prefijo de la categor√≠a
                if category_prefix in subcat_text.lower():
                    self.logger.info(f"‚úÖ Subcategor√≠a relevante encontrada: {subcat_text}")
                    # Hacer clic en la subcategor√≠a
                    await subcat.click()
                    await page.wait_for_timeout(2000)
                    # Esperar a que se carguen los productos iniciales
                    await self.await_products_loaded(page)
                    # Realizar scroll infinito para cargar TODOS los productos
                    productos_cargados = await self.scroll_to_load_all_products(page)
                    self.logger.info(f"‚úÖ Total productos cargados en subcategor√≠a '{subcat_text}': {productos_cargados}")
                    # Obtener el HTML actualizado despu√©s del scroll
                    content = await page.content()
                    from scrapy.http import HtmlResponse
                    updated_response = HtmlResponse(
                        url=page.url,
                        body=content,
                        encoding='utf-8'
                    )
                    # Parsear todos los productos
                    for item in self.parse_products(updated_response, subcat_text):
                        yield item
                    # Volver al men√∫ para la siguiente subcategor√≠a
                    await page.click(inkafarma.SELECTOR_CATEGORIES_MENU_BUTTON)
                    await page.wait_for_timeout(1000)
            # Si no se encuentra ninguna subcategor√≠a relevante, loggear
            self.logger.info("üîç Procesamiento de subcategor√≠as completado")
        except Exception as e:
            self.logger.error(f"‚ùå Error navegando subcategor√≠as: {e}")
            self.logger.info("üîÑ Navegaci√≥n de subcategor√≠as fall√≥, continuando con scraping normal...")
            await self.await_products_loaded(page)
            productos_cargados = await self.scroll_to_load_all_products(page)
            self.logger.info(f"‚úÖ Scraping normal completado: {productos_cargados} productos cargados")
            content = await page.content()
            from scrapy.http import HtmlResponse
            updated_response = HtmlResponse(
                url=str(page.url),
                body=content,
                encoding='utf-8'
            )
            # Extraer categor√≠a de la URL para usar como subcategor√≠a
            category_name = self.extract_category_from_url(str(page.url))
            for item in self.parse_products(updated_response, category_name):
                yield item

    async def process_subcategory_products(self, page, subcategory_name):
        """Procesar productos de una subcategor√≠a espec√≠fica"""
        try:
            # Verificar si esta subcategor√≠a tambi√©n tiene >= 250 productos
            subcategory_count = await self.get_product_count(page)
            self.logger.info(f"üìä Subcategor√≠a '{subcategory_name}': {subcategory_count} productos")
            
            if subcategory_count >= 250:
                # Si la subcategor√≠a tambi√©n tiene muchos productos, podr√≠a tener sub-subcategor√≠as
                # Por ahora, procesamos normalmente pero se puede extender recursivamente
                self.logger.warning(f"‚ö†Ô∏è Subcategor√≠a '{subcategory_name}' tiene {subcategory_count} productos (>=250)")
            
            # Esperar a que se carguen los productos
            await self.await_products_loaded(page)
            
            # Realizar scroll para cargar todos los productos
            productos_cargados = await self.scroll_to_load_all_products(page)
            self.logger.info(f"‚úÖ Subcategor√≠a '{subcategory_name}': {productos_cargados} productos cargados")
            
            # Obtener el HTML y procesar productos
            content = await page.content()
            from scrapy.http import HtmlResponse
            updated_response = HtmlResponse(
                url=page.url,
                body=content,
                encoding='utf-8'
            )
            
            # Parsear productos con la subcategor√≠a como contexto
            for item in self.parse_products(updated_response, subcategory_name):
                yield item
                
        except Exception as e:
            self.logger.error(f"‚ùå Error procesando productos de subcategor√≠a '{subcategory_name}': {e}")

    async def await_products_loaded(self, page):
        try:
            # Esperar a que aparezcan los productos iniciales con timeout m√≠nimo
            await page.wait_for_selector(inkafarma.SELECTOR_PRODUCTO_CARD, timeout=8000)
            self.logger.info("‚úÖ Productos iniciales cargados")
            
            # Esperar tiempo m√≠nimo para asegurar renderizado completo
            await page.wait_for_timeout(1500)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è No se pudieron cargar productos iniciales: {e}")
            # Esperar tiempo m√≠nimo y continuar
            await page.wait_for_timeout(2000)

    async def scroll_to_load_all_products(self, page):
        previous_height = -1
        scroll_attempts = 0
        max_attempts = 55
        stable_attempts = 0
        max_stable_attempts = 3
        
        while scroll_attempts < max_attempts:
            try:
                # Obtener la altura actual del contenedor de productos (sin esperar al selector)
                current_height = await page.evaluate(f"""
                    (() => {{
                        const container = document.querySelector('{inkafarma.SELECTOR_PRODUCTOS_CONTAINER}');
                        return container ? container.scrollHeight : document.body.scrollHeight;
                    }})()
                """)
                
                self.logger.info(f"üìè Scroll {scroll_attempts + 1}: Altura del contenedor: {current_height}px")
                
                # Si la altura no cambi√≥, verificar estabilidad
                if current_height == previous_height:
                    stable_attempts += 1
                    self.logger.info(f"üîÑ Altura estable {stable_attempts}/{max_stable_attempts}")
                    
                    if stable_attempts >= max_stable_attempts:
                        self.logger.info("üõë La altura del contenedor se estabiliz√≥ completamente. Scroll finalizado.")
                        break
                else:
                    # Si cambi√≥ la altura, resetear contador de estabilidad
                    stable_attempts = 0
                
                previous_height = current_height
                
                # Hacer scroll hasta el final de la p√°gina actual
                self.logger.info("üìú Haciendo scroll hasta el final...")
                await page.evaluate(f"window.scrollTo(0, {current_height})")
                
                # Esperar tiempo reducido entre scrolls
                await page.wait_for_timeout(1000)
                scroll_attempts += 1
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Error durante scroll {scroll_attempts + 1}: {e}")
                # Si falla el contenedor, intentar scroll b√°sico
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(1000)
                scroll_attempts += 1
        
        # Obtener el conteo final de productos solo despu√©s de que el tama√±o se estabilice
        try:
            productos_finales = await page.locator(inkafarma.SELECTOR_PRODUCTO_CARD).count()
            self.logger.info(f"üèÅ Scroll finalizado despu√©s de {scroll_attempts} intentos: {productos_finales} productos cargados")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Error obteniendo conteo final de productos: {e}")
            productos_finales = 0
        
        return productos_finales

    async def parse_products(self, response, category_name="farmacia", subcategory_name=None):
        """
        Extrae productos √öNICAMENTE despu√©s de que el scroll se haya completado y la p√°gina est√© estable.
        Utiliza el selector correcto: //fp-filtered-product-list//fp-product-large (Card de producto)
        """
        # Obtener el objeto page de Playwright
        page = response.meta.get("playwright_page")
        if not page:
            self.logger.error("‚ùå No hay p√°gina de Playwright disponible")
            return
        
        # Realizar scroll completo para cargar todos los productos
        await self.scroll_to_load_all_products(page)
        
        # Actualizar el response con el contenido despu√©s del scroll
        updated_content = await page.content()
        response = response.replace(body=updated_content.encode('utf-8'))
        
        # Usar XPath para seleccionar todos los fp-product-large dentro de fp-filtered-product-list
        productos = response.xpath("//fp-filtered-product-list//fp-product-large")
        
        context = f" en subcategor√≠a '{subcategory_name}'" if subcategory_name else ""
        self.logger.info(f"üîç Procesando {len(productos)} productos encontrados{context} (DESPU√âS del scroll completo)")
        
        for i, producto in enumerate(productos):
            try:
                item = ScraperItem()
                
                # Extraer nombre del producto
                nombre_elem = producto.css(inkafarma.SELECTOR_PRODUCTO_NOMBRE + "::text").get()
                nombre = nombre_elem.strip() if nombre_elem else ""
                
                # Extraer presentaci√≥n/cantidad  
                presentacion_elem = producto.css(inkafarma.SELECTOR_PRODUCTO_PRESENTACION + "::text").get()
                presentacion = presentacion_elem.strip() if presentacion_elem else ""
                
                # Crear nombre completo como solicita el usuario: "Nombre - Presentaci√≥n"
                if nombre and presentacion:
                    nombre_completo = f"{nombre} - {presentacion}"
                else:
                    nombre_completo = nombre or "Sin nombre"
                
                item['name'] = nombre_completo
                
                # Extraer precio
                precio_elem = producto.css(inkafarma.SELECTOR_PRODUCTO_PRECIO + "::text").get()
                precio_text = precio_elem.strip() if precio_elem else "0"
                
                # Limpiar y procesar precio (puede venir como "S/ 5.20S/ 2.20")
                # Separar precios m√∫ltiples y tomar el precio m√°s bajo (oferta)
                precios = []
                if 'S/' in precio_text:
                    # Dividir por 'S/' y limpiar cada precio
                    partes = precio_text.split('S/')
                    for parte in partes:
                        if parte.strip():
                            # Remover todo excepto n√∫meros y punto decimal
                            precio_limpio = inkafarma.REGEX_SOLO_NUMEROS.sub('', parte.strip())
                            try:
                                if precio_limpio:
                                    precios.append(float(precio_limpio))
                            except ValueError:
                                continue
                
                # Tomar el precio m√°s bajo (generalmente el precio de oferta)
                precio = min(precios) if precios else 0.0
                
                item['price'] = precio
                
                # Calcular precio unitario basado en la presentaci√≥n
                unit_price = self.calculate_unit_price(precio, presentacion)
                item['unit_price'] = unit_price
                
                # Extraer cantidad y unidad de la presentaci√≥n
                quantity, unit_type = self.extract_quantity_and_unit(presentacion)
                item['total_unit_quantity'] = quantity
                item['unit_type'] = unit_type
                
                # Asignar categor√≠a y subcategor√≠a seg√∫n especificaci√≥n
                # Siempre usar "farmacia" como categor√≠a principal
                item['category'] = category_name if category_name else "farmacia"
                item['sub_category'] = subcategory_name if subcategory_name else None
                
                # Informaci√≥n comercial
                item['comercial_name'] = inkafarma.COMERCIAL_NAME
                item['comercial_id'] = inkafarma.COMERCIAL_ID
                
                yield item
                
            except Exception as e:
                self.logger.error(f"‚ùå Error procesando producto {i+1}: {e}")
                continue
        
        self.logger.info(f"‚úÖ Scraping completado: {len(productos)} productos extra√≠dos{context} (despu√©s del scroll completo)")
    
    def extract_category_from_subcategory(self, subcategory_name):
        """
        Extrae la categor√≠a principal de una subcategor√≠a usando la estructura JSON cuando est√© disponible
        """
        try:
            # Priorizar b√∫squeda en la estructura JSON
            if self.structure_data and isinstance(self.structure_data.get('categories'), dict):
                for categoria_key, categoria_obj in self.structure_data['categories'].items():
                    subcats = categoria_obj.get('subcategories', {}) or {}
                    # Buscar por slug (clave) o por nombre visible
                    if subcategory_name in subcats:
                        return categoria_key
                    for subkey, subobj in subcats.items():
                        name = subobj.get('name', '')
                        if name and subcategory_name.lower() == name.lower():
                            return categoria_key

            # Si no se encuentra en JSON, intentar heur√≠sticas sobre el texto
            if 'packs' in subcategory_name.lower():
                return 'inka-packs'
            elif 'bebe' in subcategory_name.lower() or 'mama' in subcategory_name.lower():
                return 'mama-y-bebe'
            elif 'dermocosmetica' in subcategory_name.lower() or 'dermocosmetica' in subcategory_name.lower():
                return 'dermatologia-cosmetica'
            elif 'suplementos' in subcategory_name.lower():
                return 'nutricion-para-todos'
            else:
                return 'farmacia'  # Categor√≠a por defecto (en min√∫scula, consistente con uso JSON)

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è No se pudo extraer categor√≠a de '{subcategory_name}': {e}")
            return 'farmacia'
    
    def calculate_unit_price(self, precio_total, presentacion):
        try:
            match = inkafarma.REGEX_CANTIDAD_PRESENTACION.search(presentacion)
            if match:
                cantidad = float(match.group(1))
                if cantidad > 0:
                    return round(precio_total / cantidad, 2)
            return precio_total
        except Exception:
            return precio_total
    
    def extract_quantity_and_unit(self, presentacion):
        """Extrae cantidad y unidad de la presentaci√≥n"""
        try:
            match = inkafarma.REGEX_CANTIDAD_PRESENTACION.search(presentacion)
            if match:
                cantidad = float(match.group(1))
                unidad = match.group(2).lower()
                return cantidad, unidad
            # Si no encuentra patr√≥n espec√≠fico, asumir 1 unidad
            return 1.0, "un"
        except Exception:
            return 1.0, "un"
    
    def extract_category_from_url(self, url):
        """Extrae la categor√≠a desde la URL"""
        try:
            # URL format: https://inkafarma.pe/categoria/categoria-nombre
            if '/categoria/' in url:
                categoria = url.split('/categoria/')[-1]
                return categoria.replace('-', ' ').title()
            return "General"
        except Exception:
            return "General"
    
    def extract_category_raw_from_url(self, url):
        """Extrae la categor√≠a desde la URL en formato raw (sin conversi√≥n) para buscar en diccionarios"""
        try:
            # URL format: https://inkafarma.pe/categoria/categoria-nombre?params
            if '/categoria/' in url:
                # Obtener parte despu√©s de /categoria/
                categoria_part = url.split('/categoria/')[-1]
                # Limpiar par√°metros de query (?param=value)
                categoria = categoria_part.split('?')[0]
                # Limpiar fragmentos (#fragment)
                categoria = categoria.split('#')[0]
                return categoria.strip()
            return "general"
        except Exception:
            return "general"